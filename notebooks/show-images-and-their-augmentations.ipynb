{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The purpose of this notebook is to show how to perform inference on quadrat test images with a tiling-based approach. As you may have have already noticed, predicting the species directly on the entire high-resolution image is a challenging task that often yields suboptimal results. The more intuitive strategy is then split the image into smaller patches, predict the species on each patch and finally aggregate the results.\n",
    "\n",
    "Here we use a simple method where predictions are thresholded based on species scores and their relative positions among the softmax output. The predictions for each tile are generated using the DINOv2-based model provided in the competition.\n",
    "This notebook can be used as a starting point for further development. Feel free to leave comments on errors or for any improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T16:18:06.743830Z",
     "iopub.status.busy": "2025-03-26T16:18:06.743496Z",
     "iopub.status.idle": "2025-03-26T16:18:06.748420Z",
     "shell.execute_reply": "2025-03-26T16:18:06.747572Z",
     "shell.execute_reply.started": "2025-03-26T16:18:06.743797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from torch.amp import autocast\n",
    "from matplotlib import pyplot as plt\n",
    "from kornia import tensor_to_image\n",
    "from kornia.contrib import extract_tensor_patches, compute_padding\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T14:17:27.920460Z",
     "iopub.status.busy": "2025-03-26T14:17:27.919915Z",
     "iopub.status.idle": "2025-03-26T14:17:27.928475Z",
     "shell.execute_reply": "2025-03-26T14:17:27.927644Z",
     "shell.execute_reply.started": "2025-03-26T14:17:27.920425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, patches, transform=None):\n",
    "        self.patches = patches.squeeze(0)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.patches.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patch = self.patches[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "        return patch\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, image_folder, patch_size=518, stride=259, transform=None, use_pad=False\n",
    "    ):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_paths = [\n",
    "            os.path.join(image_folder, f) for f in os.listdir(image_folder)\n",
    "        ]\n",
    "        self.transform = transform\n",
    "        self.use_pad = use_pad\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image).unsqueeze(0)\n",
    "\n",
    "        h, w = image.shape[-2:]\n",
    "\n",
    "        if self.use_pad:\n",
    "            pad = compute_padding(\n",
    "                original_size=(h, w), window_size=self.patch_size, stride=self.stride\n",
    "            )\n",
    "            patches = extract_tensor_patches(\n",
    "                image, self.patch_size, self.stride, padding=pad\n",
    "            )\n",
    "        else:\n",
    "            patches = extract_tensor_patches(image, self.patch_size, self.stride)\n",
    "\n",
    "        return patches, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T14:20:30.685780Z",
     "iopub.status.busy": "2025-03-26T14:20:30.685466Z",
     "iopub.status.idle": "2025-03-26T14:20:42.311518Z",
     "shell.execute_reply": "2025-03-26T14:20:42.310675Z",
     "shell.execute_reply.started": "2025-03-26T14:20:30.685757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PROJECT_PATH = \"/mnt/storage1/shared_data/plant_clef_2025/\"\n",
    "DATA_FOLDER_PATH = f\"{PROJECT_PATH}data/\"\n",
    "TEST_DATA_PATH = f\"{DATA_FOLDER_PATH}plant_clef_2025_test/\"\n",
    "METADATA_PATH = f\"{DATA_FOLDER_PATH}metadata/\"\n",
    "\n",
    "df_species_ids = pd.read_csv(f\"{METADATA_PATH}species_ids.csv\")\n",
    "\n",
    "df_metadata = pd.read_csv(\n",
    "    f\"{METADATA_PATH}PlantCLEF2024_single_plant_training_metadata.csv\",\n",
    "    sep=\";\",\n",
    "    dtype={\"partner\": str},\n",
    ")\n",
    "class_map = df_species_ids[\n",
    "    \"species_id\"\n",
    "].to_dict()  # dictionary to map the species model Id with the species Id\n",
    "\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T14:17:40.285244Z",
     "iopub.status.busy": "2025-03-26T14:17:40.284927Z",
     "iopub.status.idle": "2025-03-26T14:17:43.129538Z",
     "shell.execute_reply": "2025-03-26T14:17:43.128570Z",
     "shell.execute_reply.started": "2025-03-26T14:17:40.285191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_FOLDER_PATH = f\"{PROJECT_PATH}models/\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = timm.create_model(\n",
    "    \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "    pretrained=False,\n",
    "    num_classes=len(df_species_ids),\n",
    "    checkpoint_path=f\"{MODEL_FOLDER_PATH}model_onlyclassifier_then_all/model_best.pth.tar\",\n",
    ")\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T14:17:43.130727Z",
     "iopub.status.busy": "2025-03-26T14:17:43.130480Z",
     "iopub.status.idle": "2025-03-26T14:17:43.134942Z",
     "shell.execute_reply": "2025-03-26T14:17:43.133972Z",
     "shell.execute_reply.started": "2025-03-26T14:17:43.130705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "model_input_size, model_mean, model_std = (\n",
    "    data_config[\"input_size\"][1],\n",
    "    data_config[\"mean\"],\n",
    "    data_config[\"std\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters:\n",
    "* batch_size: size of batch of testing images\n",
    "* top_k: keep best top_k results for each patch\n",
    "* min_score: keep only classes with a score higher than min_score\n",
    "* patch_size: size of patches. We will be using the DinoV2 image input size that is 518.\n",
    "* stride: overlapping stride. We will be using half of the patch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple of tiling on single image. Let's load a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T14:17:43.148615Z",
     "iopub.status.busy": "2025-03-26T14:17:43.148283Z",
     "iopub.status.idle": "2025-03-26T14:17:43.766607Z",
     "shell.execute_reply": "2025-03-26T14:17:43.765601Z",
     "shell.execute_reply.started": "2025-03-26T14:17:43.148574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_random_images_from_directory(directory: str, num_images: int = 10) -> list[str]:\n",
    "    # List all files in the directory\n",
    "    all_files = os.listdir(directory)\n",
    "    # Randomly select num_images from the list\n",
    "    selected_images = random.sample(all_files, min(num_images, len(all_files)))\n",
    "    # Return full paths\n",
    "    return [os.path.join(directory, img) for img in selected_images]\n",
    "\n",
    "\n",
    "images_paths = get_random_images_from_directory(TEST_DATA_PATH, num_images=10)\n",
    "images = [Image.open(path) for path in images_paths]\n",
    "\n",
    "# plot the images\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as ttransforms\n",
    "\n",
    "\n",
    "def get_random_data_augmentation() -> ttransforms.transforms:\n",
    "    return ttransforms.Compose(\n",
    "        [\n",
    "            ttransforms.RandomResizedCrop(size=(518, 518)),\n",
    "            ttransforms.RandomHorizontalFlip(),\n",
    "            ttransforms.RandomVerticalFlip(),\n",
    "            ttransforms.RandomPerspective(distortion_scale=0.2),\n",
    "            ttransforms.RandomRotation(20),\n",
    "            ttransforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the images\n",
    "aug = get_random_data_augmentation()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, image in enumerate(images):\n",
    "    aug_image = aug(image)\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(aug_image)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the 64 patches extracted with a size of 518 (the same of model input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "model_input_size, model_mean, model_std = (\n",
    "    data_config[\"input_size\"][1],\n",
    "    data_config[\"mean\"],\n",
    "    data_config[\"std\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters:\n",
    "* batch_size: size of batch of testing images\n",
    "* top_k: keep best top_k results for each patch\n",
    "* min_score: keep only classes with a score higher than min_score\n",
    "* patch_size: size of patches. We will be using the DinoV2 image input size that is 518.\n",
    "* stride: overlapping stride. We will be using half of the patch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "min_score = 0.1\n",
    "top_k_tile = 2\n",
    "patch_size = model_input_size\n",
    "stride = int(model_input_size) * 2\n",
    "use_pad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_tensor = T.ToTensor()\n",
    "\n",
    "image_tensor = image_to_tensor(image).unsqueeze(0)\n",
    "h, w = image_tensor.shape[-2:]\n",
    "\n",
    "pad = compute_padding(original_size=(h, w), window_size=patch_size, stride=stride)\n",
    "\n",
    "patches = extract_tensor_patches(image_tensor, patch_size, stride, padding=pad)\n",
    "print(f\"Shape of image tiles = {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T14:17:43.845619Z",
     "iopub.status.busy": "2025-03-26T14:17:43.845336Z",
     "iopub.status.idle": "2025-03-26T14:17:47.127629Z",
     "shell.execute_reply": "2025-03-26T14:17:47.126727Z",
     "shell.execute_reply.started": "2025-03-26T14:17:43.845596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    int(math.sqrt(patches.shape[1])), int(math.sqrt(patches.shape[1]))\n",
    ")\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(len(patches[0])):\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].imshow(tensor_to_image(patches[0][i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T14:17:47.128674Z",
     "iopub.status.busy": "2025-03-26T14:17:47.128444Z",
     "iopub.status.idle": "2025-03-26T14:17:47.274507Z",
     "shell.execute_reply": "2025-03-26T14:17:47.273601Z",
     "shell.execute_reply.started": "2025-03-26T14:17:47.128655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example of a single patch\n",
    "plt.imshow(tensor_to_image(patches[0][1]))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
