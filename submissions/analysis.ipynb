{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a10e6b3",
   "metadata": {},
   "source": [
    "# Analysing the correlation between score and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b176150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcc136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dicts(dict1, dict2):\n",
    "    combine = dict1.copy()\n",
    "    for key, value in dict2.items():\n",
    "        if key in combine and isinstance(combine[key], dict):\n",
    "            combine[key] = combine_dicts(combine[key], value)\n",
    "        else:\n",
    "            combine[key] = value\n",
    "    return combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG_PATH = \"./default_config.yaml\"\n",
    "\n",
    "with open(DEFAULT_CONFIG_PATH, \"r\") as file:\n",
    "    default_config = yaml.safe_load(file)\n",
    "\n",
    "DATA_FOLDER = \"./data\"\n",
    "all_files = {\n",
    "    os.path.splitext(f)[0]\n",
    "    for f in os.listdir(DATA_FOLDER)\n",
    "    if os.path.isfile(os.path.join(DATA_FOLDER, f))\n",
    "}\n",
    "\n",
    "print(f\"Found {len(all_files)} submissions.\")\n",
    "\n",
    "submission_data = {}\n",
    "\n",
    "for submission_name in all_files:\n",
    "    run_config_file = os.path.join(DATA_FOLDER, f\"{submission_name}.yaml\")\n",
    "    run_submission_file = os.path.join(DATA_FOLDER, f\"{submission_name}.csv\")\n",
    "\n",
    "    with open(run_config_file, \"r\") as run_file:\n",
    "        run_config = combine_dicts(default_config, yaml.safe_load(run_file))\n",
    "\n",
    "    with open(run_submission_file, mode=\"r\") as submission_file:\n",
    "        csv_reader = csv.DictReader(submission_file)\n",
    "        run_submission = {row[\"quadrat_id\"]: row[\"species_ids\"] for row in csv_reader}\n",
    "\n",
    "    submission_data[submission_name] = {\n",
    "        \"config\": run_config,\n",
    "        \"submission\": run_submission,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter submissions we don't want to see\n",
    "\n",
    "# submission_data = {k: v for k, v in submission_data.items() if v['config']['models']['checkpoint_file'] == '5heads_1layer'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfbe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "for submission_name, data in submission_data.items():\n",
    "    if data[\"config\"][\"prediction\"][\"method\"] != \"top_k_tile\":\n",
    "        continue\n",
    "    scores[data[\"config\"][\"prediction\"][\"top_k_tile\"][\"k\"]].append(\n",
    "        data[\"config\"][\"score\"]\n",
    "    )\n",
    "\n",
    "scores = dict(sorted(scores.items()))\n",
    "\n",
    "plt.boxplot(scores.values(), positions=scores.keys())\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Boxplot of Scores by top k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "for submission_name, data in submission_data.items():\n",
    "    scores[sorted(data[\"config\"][\"prediction\"][\"tiling\"][\"scales\"]).__str__()].append(\n",
    "        data[\"config\"][\"score\"]\n",
    "    )\n",
    "\n",
    "scores = dict(sorted(scores.items()))\n",
    "\n",
    "plt.boxplot(\n",
    "    scores.values(),\n",
    "    positions=range(len(scores.keys())),\n",
    "    tick_labels=list(scores.keys()),\n",
    ")\n",
    "plt.xlabel(\"scales\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Boxplot of Scores by scales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "for submission_name, data in submission_data.items():\n",
    "    scores[sorted(data[\"config\"][\"prediction\"][\"tiling\"][\"overlaps\"]).__str__()].append(\n",
    "        data[\"config\"][\"score\"]\n",
    "    )\n",
    "\n",
    "scores = dict(sorted(scores.items()))\n",
    "\n",
    "plt.boxplot(\n",
    "    scores.values(),\n",
    "    positions=range(len(scores.keys())),\n",
    "    tick_labels=list(scores.keys()),\n",
    ")\n",
    "plt.xlabel(\"overlaps\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Boxplot of Scores by overlaps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88cb6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "for submission_name, data in submission_data.items():\n",
    "    if data[\"config\"][\"prediction\"][\"method\"] != \"top_k_tile\":\n",
    "        continue\n",
    "    scores[data[\"config\"][\"prediction\"][\"top_k_tile\"][\"min_score\"]].append(\n",
    "        data[\"config\"][\"score\"]\n",
    "    )\n",
    "\n",
    "scores = dict(sorted(scores.items()))\n",
    "\n",
    "plt.boxplot(\n",
    "    scores.values(),\n",
    "    positions=range(len(scores.keys())),\n",
    "    tick_labels=list(scores.keys()),\n",
    ")\n",
    "plt.xlabel(\"min_score\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Boxplot of Scores by top k min score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58de77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "for submission_name, data in submission_data.items():\n",
    "    if data[\"config\"][\"prediction\"][\"method\"] != \"BMA\":\n",
    "        continue\n",
    "    scores[data[\"config\"][\"prediction\"][\"BMA\"][\"z_score_threshold\"]].append(\n",
    "        data[\"config\"][\"score\"]\n",
    "    )\n",
    "\n",
    "scores = dict(sorted(scores.items()))\n",
    "\n",
    "if len(scores) != 0:\n",
    "    plt.boxplot(\n",
    "        scores.values(),\n",
    "        positions=range(len(scores.keys())),\n",
    "        tick_labels=list(scores.keys()),\n",
    "    )\n",
    "    plt.xlabel(\"z_score\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Boxplot of Scores by BMA z-score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bdc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "for submission_name, data in submission_data.items():\n",
    "    scores[data[\"config\"][\"prediction\"][\"filter_genus\"]].append(data[\"config\"][\"score\"])\n",
    "\n",
    "scores = dict(sorted(scores.items()))\n",
    "\n",
    "plt.boxplot(\n",
    "    scores.values(),\n",
    "    positions=range(len(scores.keys())),\n",
    "    tick_labels=list(scores.keys()),\n",
    ")\n",
    "plt.xlabel(\"filter_genus\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Boxplot of Scores by filter_genus\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "for submission_name, data in submission_data.items():\n",
    "    scores[data[\"config\"][\"prediction\"][\"use_genus_and_family\"]].append(\n",
    "        data[\"config\"][\"score\"]\n",
    "    )\n",
    "\n",
    "scores = dict(sorted(scores.items()))\n",
    "\n",
    "plt.boxplot(\n",
    "    scores.values(),\n",
    "    positions=range(len(scores.keys())),\n",
    "    tick_labels=list(scores.keys()),\n",
    ")\n",
    "plt.xlabel(\"use_genus_and_family\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Boxplot of Scores by use_genus_and_family\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "for submission_name, data in submission_data.items():\n",
    "    scores[data[\"config\"][\"models\"][\"checkpoint_file\"]].append(data[\"config\"][\"score\"])\n",
    "\n",
    "scores = dict(sorted(scores.items()))\n",
    "\n",
    "plt.boxplot(\n",
    "    scores.values(),\n",
    "    positions=range(len(scores.keys())),\n",
    "    tick_labels=list(scores.keys()),\n",
    ")\n",
    "plt.xlabel(\"checkpoint_file\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Boxplot of Scores by checkpoint_file\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "for submission_name, data in submission_data.items():\n",
    "    scores[data[\"config\"][\"prediction\"][\"filter_species_threshold\"]].append(\n",
    "        data[\"config\"][\"score\"]\n",
    "    )\n",
    "\n",
    "scores = dict(sorted(scores.items()))\n",
    "\n",
    "plt.boxplot(\n",
    "    scores.values(),\n",
    "    positions=range(len(scores.keys())),\n",
    "    tick_labels=list(scores.keys()),\n",
    ")\n",
    "plt.xlabel(\"filter_species_threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Boxplot of Scores by filter_species_threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"submission\"])\n",
    "\n",
    "prediction_lengths = {}\n",
    "for submission_name, data in submission_data.items():\n",
    "    prediction_lengths[data[\"config\"][\"score\"]] = [\n",
    "        len(pred.split(\" \")) for pred in data[\"submission\"].values()\n",
    "    ]\n",
    "\n",
    "print(prediction_lengths)\n",
    "\n",
    "prediction_lengths = dict(sorted(prediction_lengths.items()))\n",
    "\n",
    "plt.boxplot(\n",
    "    prediction_lengths.values(),\n",
    "    positions=range(len(prediction_lengths.keys())),\n",
    "    tick_labels=list(prediction_lengths.keys()),\n",
    ")\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"len of predictions\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Boxplot prediction lengths by score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
